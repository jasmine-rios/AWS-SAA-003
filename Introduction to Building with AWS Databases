# Module 1: AWS Well-Architected Framework 

Follow notes from other file in this repo about the framework

# Module 2: Data Types

- Structured Data
- Unstructed Data
- Semi-structured Data

## Structured Data

## Unstructured Data

## Semi-Structured Data

# AWS Database Services

## 

# Module 4: Relational Databases:

## Relational Databases Introduction

## Amazon Relational Database Service (Amazon RDS)

## Amazon Aurora

# Module 5: Nonrelational Databases

## Nonrelational Databases Introduction

## Amazon DynamoDB

## Amazon Keyspaces (for Apache Cassandra)

## Amazon Timestream

Amazon Timestream is a fast, scalable serverless time-series database for Internet of Things (IoT) and operational applications

Makes it easy to store and analyze trillions of events a day

Kepps recent data in memory and moves historical data to cost-optimized storage tier based upon user-defined policies

Has a purpose-built query engine that allows to analyze this data without needing to specify if the data is in memory or a cost-optimized tier

Has a built-in-time-series analytics funtion to help identify trends and patterns in near real-time

It is serverless and automatically scales up and down to adjust for capacity and performance

### High Performance at Low Cost

Timestream is designed to facilitate interactive and affortable real-time analytics

Has product features such as scheduled queries, mult-measure records, and data storage tiering

You can proccess, store, and analyze your time-series data at a fraction of the cost of existing time-series solutions

Helps you derieve faster and more-affortable insights from your data so you can continue to make more data-driven business decisions

### Serverless with Auto Scaling

This is serverless which means no servers to manage and no capacity to provision

Gives you the scale to process trillions of event and millions of queries a day

Automatically scales to adjust for capacity

### Data lifecycle management

Simplifies the complex process of data management

Offers storage tiering with a memory store for recent data and magnetic store for historical data

Automates transfer of data from memory store to magnetic store based on user-configurable policies

### Simplifed data access

You no longer have to use disparate tools to access recent and historical data

Has a purpose-build query engine that transparently accessses and combines data across storage tiers without you having to specify the data location

### Purpose-built for Time Series

You can analyze time-series data using SQL using built-in time series functions for smoothing, approximation, and interpolation

Also supports advanced aggregates, windows functions, and complex data types such as arrays and rows

### Always Encrypted

Ensures data is always encrypted at rest or in transit

You can also specifiy AWS Key Management (AWS KMS) customer managed key for encrypting data in the magnetic store

### Use Case: IoT sensor data capture architecture

1. You use *Amazon SQS* to have sensor data sit in quenue to be processed by *AWS Lambda*

2. This event triggers the *AWS Lamabda* to initiate a function that loads the message into the Timestream table for data storage

3. *Amazon Timestream* serves as a first-stage repository for all messages sent to the *Amazon SQS* quenue

4. *Amazon EMR* gathers this data from Timestream and processes it then stores it in a *S3 data lake*

5. Once stored in the data lake, *Amazon Athena* can query the data

## Amazon Quantum Ledger Database (Amazon QLDB)

### What is Amazon QLDB?

It is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log 

This log is owned by a centeral trusted authority

QLDB tracks each and every application data change and maintains a complete and verifiable history of changes over time.

Storage is billed per GB per month

IOs consumed are billed per million requests

Data transfer charges are based on data transfer IN and data transfer OUT

No charge between QLBD interacting with other AWS services within the same region

### Use Case: Delivering Cryptographically Secure and Verifiable Medical Data

Medical companies use QLDB to maintain their records.

It is not as optimized as others for one-time search, analytics, and reporting, although these are possible

This is how the architecture is configured:

1. *ElastiCache for Memcached* caches user session data to provide near real-time response times
2. *AWS Elastic Benstalk* deploys and runs the frontend and application logic
    It offloads the operational work of deployent, patching, and scaling
3. *Amazon QLDB* maintains and stores unfalsable an crypto verifiable records of medical histories
    Successful transactions are streamed to Amazon RDS database fir searching and analytics
4. Data is sent from QLDB and then exported by *Kinesis Data Streams* to Amazon RDS to be queried 
5. Since Amazon QLDB is not optimized for ad-hoc search, reporting, and analytics, the successful transactions are placed into *Amazon RDS* instead
   Amazon QLDB remains the database used for a system of record.

## Amazon ElastiCache

### What is Amazon ElastiCache?

It offers managed Redis and memcached distributed in-memory caches using nodes for frequent reads instead of going to the database

Seamlessly deploy, run, and scale popular open source-compatible, in-memory data stores

Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores

Pay only for what you use, no minimum fee

ElastiCache provides storage for one database snapshot at no charge

Each additional snapshot is charged per gigabyte per month

You are only charged for the data transfer in or out of your Amazon EC2 instance

There is no ElastiCache data transfer charge for traffic in or out of the ElastiCache node itself

ElastiCache supports two nodes types:
    - On-demand
    - Reserved

### On-demand Nodes

You pay for memory capacity by the hour with no long-term commitments

Pricing is per node/hour consumed from the time you launch a node until you terminate it

### Reserved Nodes

You save up to 75% over on-demand rates by commiting for a one or three-year term 
ElatiCache supports two engines:
    - Redis
    - Memcached

### Redis

Redis supports complex data types, data replication, and high avaliability 

It is ideal for sesion caching, full page caching, message quenue applications, leader boards and much more

### Memcached

Is for data that is relatively small and static

For example, a static HTML page or JavaScript and CSS files

Performance is increased instead of waiting for database and Disk I/O

### Use Case: Word Press Database Cache Architecture

When database access is causing performance problems for an application, an in-memory cache can improve latency, increase throughput, and ease the load on the database

For this use case, this architecture is one way you can serve cache items in less than a millisecound and scale cost effectively

1. Mobile client connects to WordPress website through *Amazon EC2*
2. The database is Auroa and uses *ElastiCache* to store data for the website
   It holds a copy of data retrieved from the database for the specified period of time
   The only time the database is accessed is when noncached data is required
3. *Amazon Aurora* stores all the data for the website

### Use Case: Scalable Distrubuted Cache Architecture

To address scalabililty and provide a shared data storage for sessions that can be accessible from any individual web sever

You can abstract the HTTP sessions from the web servers themselves by storing them in a remote cache like ElastiCache

1. *Elastic Load Balancer* distributes traffic between two Amazon EC2 web servers
2. *Amazon EC2* instances are hosyomg the web application and are configured in an Auto Scaling group across two AZs
3. *ElastiCache* distributes the session information across four cache nodes in two different AZs

## Amazon MemoryDB for Redis
Amazon MemoryDB for Redis is a Redis-compatible, durable, in-memory database service that delivers ultra-fast performance. It is purpose-built for modern applications with microservices architectures.

MemoryDB is compatible with Redis, a popular open-source datastore

giving customers the ability to quickly build applications using the same flexible and friendly Redis data structures, application programming interfaces (APIs), and commands that they already use today. 

With MemoryDB, all your data is stored in memory, so you can achieve microsecond read and single-digit millisecond write latency and high throughput

MemoryDB also stores data durably across multiple Availability Zones using a distributed transactional log to facilitate fast failover, database recovery, and node restarts

Delivering both in-memory performance and Multi-AZ durability,

Amazon MemoryDB can be used as a high-performance primary database for your microservices applications, eliminating the need to separately manage both a cache and durable database

### Build Web and Mobile Applications
Use versatile Redis data structures such as 
        streams, lists, and sets 
to build content data stores, chat and message queues, and geospatial indexes for demanding, data-intensive web and mobile applications that require low latency and high throughput

### Quickly Access Customer Data for Retail

Deliver personalized customer experiences and manage user profiles, preferences, and inventory tracking and fulfillment with microsecond read and single-digit millisecond write latency

### Develop Online Games

Build player data stores, session history, and leaderboards for gaming applications that require massive scale, low latency, and high concurrency to make real-time updates

### Stream Media and Entertainment

Run high-concurrency streaming data feeds to ingest user activity and support millions of requests per day for media and entertainment applications

### Use Case: Redis Data Structures for Microservices

Redis is an extremely popular choice for low-latency, high-throughput requirements within containerized workloads

Redis has become a preferred in-memory layer for many enterprise organizations and startups

Here, Python microservices on Amazon ECS and MemoryDB for Redis as the backend primary database

1. Clients access the application from a browser
   The *application load balancer* is the first step in the architecture. From their data is sent to application containers
2. Container-based microservices are deployed to *Amazon ECS*
3. *AWS Fargate* is used to help create and update application that need to be built
4. The application code is deployed into container images and hosted in *Amazon ECR* and configured using an ECS task definition
5. The microservices interact with *MemoryDB for Redisz8 using the open-source redis-py-cluster aware client
   The application code also usees Flask to configure a simple API
   Although the application is built using a shared MemoryDB for Redis cluster, microservices patterns allow independent clusters to be used in a database-per-service model
   MemoryDB for Redis supports ACLs which can restrict user access to certain keys, and commmands based on an access string
6. The *CI/CD pipeline* is the final step, making continous updates and improvements to the application and the ECR

# Module 6: Data Access and Analysis

## Data Access and Analysis Introduction

### Data Access and Analysis

Outside your database needs, there are services that AWS offers to help with data access and analysis
These two services, while they don't fall under a relational or nonrelational database category, are essential tools

### Amazon Redshift

It is a fast, simple, and cost-effective data warehouse

Tens of thousands of customers today rely on Amazon Redshift to analyze exabytes of data and run complex analytical queries, making it the most widely used cloud data warehouse

Run and scale analytics in secounds on all your data without having to manage your data warehouse infrastructure

### Amazon Athena

Amazon Athena is an interactive query service that streamlines analyzing data in Amazon S3 using standard structured query language (SQL)

It is serverless and you only pay for the queries that you run

## Amazon Redshift

### What is Amazon Redshift?

It is a fast, scalable data warehouse that makes it simple and cost effective to analyze all your data across your data warehouse and data lake

It uses machine learning, massively parallel query execution, and columnar storage on high-performance disk

### Use Case: Rich Data Platform Architecture

Redshift is a perfect respository for analytical data

The question becomes how to get quality data into the database with speed, efficency, and accuracy

To solve this case, one option is to use this architecture for building out a rich data platform pulling data from multiple data sources

1. *Amazon RDS* houses a large volume of transactional data that is vital to the analytical processes of the buisness

2. *Amazon S3* bucket contains architeced data from hundreds of data repositories

3. *AWS Glue* is pulling data from both Amazon RDS and AmaazonS3, transforming the data into analyical results, then loading those results into the Amazon Redshift data warehouse

4. *Amazon Redshift* is the final respository for the analytical data
    Services such as Amazon QuickSight can access the respository for visualization and analysis of the data

### Use Case: Event-driven Data Analysis Architecture

With the speed of data generation increasing all the time. the speed of data analysis and reporting must increase at the same pace

This archiecture is one way to solve this use case and used to create an event-driven data analysis solution

1. *Amazon Kinesis Data Firehose* gathers data from on-premises data servers
    This processs initiates a Lambda function

2. The *AWS Lambda* function takes the data from the Kinesis Firehose stream and loads it into an Amazon S3 data lake

3. *Amazon S3 Data Lake* stores all the data gathered by Kinesis Data Firehose

4. The *Amazon Redshift* cluster already has been loaded with the analyical data

5. *Amazon Redshift Spectrum* is used to query both the data within the Amazon Redshift tables and the data gathered from the Lambda function

6. *Amazon Quicksight* uses the Amazon Redshift cluster as a data source
   The reports and dashboard written in Quicksight can refresh and load all new records in real time

## Amazon Athena

Athena is an interactive query service that facilitates analyzing data in Amazon S3 using standard SQL

It is serverless and you only pay for the queries that you run

It is straightfoward to use

Point to your data in Amazon S3, define the schema, and start querying using Standard SQL

Most results are delivered within seconds

With Athena, there is no need for complex ETL jobs to prepare your data for analysis

This helps anyone with SQL skills to quickly analyze large-scale datasets

### Pay Per Query

You only pay for the queries you run

You are charged $5 per terabyte scanned by your queries

You can save from 30-90% on your per-query costs and get better performance by compressing, partioning, and converting your data into columnar formats

Athena queries data diretvly in S3 and there are no additional storage charges beyond S3

### Open, Powerful, Standard

Athena uses Presto with ANSI SQL support and works with a variety of standard data formats, including CSV, JSON, ORC, Avro, and Parquet

It is ideal for quick, one-time querying but it can also handle complex analysis, including large joins, windows functions, and arrays

It is highly avaliable and runs queries using compute resources across multiple facilities and devices in each facility

Athena uses Amazon S3 as its underlying data store, making your data highly avaliable and durable

### Fast...Really Fast

With Athena, you don't have to worry about not having enough compute resources to get fast, interactive query performance

Athena automatically runs queries in parallel, so most results come back within seconds

### Use Case: Building for Athena Federated Query

One major case for Amazon Athena is using Athena Federated Query so customers can submit a single SQL query and analyze data from multiple sources running on-premises or hosted on the cloud

Athena runs federated queries using data source connectors that run on AWS Lambda

AWS has open-sourced Athena data source connectors for Amazon DynamoDB, Apache Hbase, Amazon DynamoDB (with MongoDB compatibility), Amazon Redshift, Amazon CloudWatch Logs, Amazon CloudWatch Metrics Insights, and JDBC-compliant relational data sources such as MySQL and PostgreSQK under the Apache 2.0 licence

1. Users submit a query using *Athena Federated Query*
   Amazon SageMaker also runs automated queries
   Those queries are then sent to an Amazon S3 bucket for data to be analyzed

2. The Federated Queies run on Athena will first go through *Lambda functions*
   The Lambda functions act as a data source connector
   You can think of a connector as an extension of the Athena query engine
   These connectors are each associated with a database and tell Athena which tables need to be read, manage parrallelism, and push down filter predicates

3. By using the data connectors from the lambda functions, each of these databases/*AWS services* (Amazon EMR, Amazon Aurora, Amazon DynamoDB, Amazon ElastiCache) 
   can be queied using Athena Federated Query

4. Redis doesn't have a schema oof its own so *AWS Glue* database and tables need to be set up so the data can be associated to the schema used in the Athena Federated Query

5. The responses from the queries are sent and stored in an *S3 bucket* as a spill-over 

# Module 7: Choosing the Right Database Activity

## Chosing the Right Database

Guess which database to use based on the scenarios below:

Q: A buisness is currently running their inventory management on premises, but is looking to move into the cloud for increased performance and scalability

A: For online transaction processing, or OLTP, and online analytical processing, or OLAP, databases using row-based indexing, we have Amazon Relational Datatbase Service, alsso called *Amazon RDS*.
Now, this service streamlines setting up, operating, and scaling a relational database in the cloud.
The service provides cost-efficent and scalable capacity while automating many time-consuming administration tasks, such as hardware provisioning, database setup, patching, and backups

Q: A buiness running a gaming website is noticing that their database is running slowly because of how rapidly they are growing. 
A: ElastiCache for Redis

Q: A buisness needs a solution that lets database engineers focus their time on customer-facing features instead of routine database maintaince and administration
A: AWS Aurora 

Q: A business needs a database that can rapidly gather customer data 
A: Amazon DynamoDB since it is key-value database and really fast and handles a lot of requests

Q: A business needs a way to load data into a warehouse and archieve in storage so they can manage costs
A: Redshift

Q: A business is working to develop an e-commerce app that specializes in fraud detection. The business needs a solution that can provide near real-time detection.
The business needs a solution that can provide near real-time detection of patterns that are defined as suspicious and indicate known fraud activity
A: Amazon Neptime for the Neptune Graph can be used in fraud detection

Q: A business is storing online profiles in which different users provide different types of information. The buines is already using MongoDB, but wants to migrate to the cloud
A: DocumentDB since it can store semi-structured data as a document and has MongoDB compatibility

# Module 8: AWS Database Migration Tools

## Database Migration

AWS offers two migration tools:

- AWS Database Migration Service (AWS DMS)
- AWS Schema Conversion Tool (AWS SCT)

Will likely use both together to complete migration

Steps in hetrogenous migration of database in AWS SCT:

1. Determine how heavy a workload will be by using AWS SCT to plan migration
2. You will have to choose a source and login to the database to connect to it
3. You will be given an excutive summary in which have details about migrating your schema and spread the responsibility to db admins, app admins, etc
4. Will give you common issues you will run into and you can go into action items to view the actual code that requires remediation

AWS SCT helps automate schema conversion and other tasks when migrating to new database engine

If you are planning a homogenous migration, you can skip AWS SCT

You can now use AWS DMS to actually migrate the data from your source database to your target database

AWS DMS is fully managed service that easily and securely migrates or replicates your database and data warehouses to AWS 

You can use AWS DMS for homo or heterogenous migrations

You can also use AWS DMS to fan out your migration like taking on-premise Oracle DB to Aurora while also connecting it to Amazon Redshift and Amazon S3 bucket
or more without any downtime

## Migrating database schemas

There are many different migration strategies. Some common migrations include on-premises DB to AWS cloud, relational db to nonrelational db, and db hosted on EC2 to Aurora

AWS DMS creates only those objects required to efficiently migrate the data

To migrate the remaining database elements and schema, you need to use other tools depending on the type of db migration

For example, migrating MS SQL DB that is on-prem would require use of native MS SQL tools to migrate the db to Amazon RDS for MS SQL

## What is AWS SCT?

AWS Schema Converson Tool (AWS SCT) makes heterogeneous database migrations predictable.

It automatically converts the source database schema and a majority of database code objects-- including views, stored procedures, and functions-- to a format compatible with the target db

Any objects that cannot be automaticaly converted are clearly marked so that they can be manually converted to complete the migarion

AWS SCT can also scan your application source code for embedded SQL statements and convert them as part of a database schema conversion project

During this process, AWS SCT performs cloud-native code optimization by converting legacy Oracle and MS SQL server functions to their equivalent AWS service

This helps you modernize the apps at the time of db migration

## What is AWS DMS?

AWS Database Migration Service (AWS DMS) helps your migrate databases to AWS quickly and securely 

The source db remains fully operational during the migration, minimizing downtime to apps that rely on the db

The AWS DMS can migrate your data to and from the most widely used commercial and open-source databases

AWS DMS supports both homo and hetero migrations and you can also continuously replicate data with low latency from any supported source to any supported target

For example, you can replicate from multiple sources to S3 to build a highly available and scalable data lake solution

You can also consolidate dbs into petabyte-scale data warehouse by streaming data to Redshift

## How do AWS SCT and AWS DMS work together?

AWS SCT enables you to convert your schema, and AWS DMS will help you migrate the actual data in your db

AWS DMS can do migration in two ways:

- full-load migration, in which you stop your system and transfer your data all at once

- Or perform ongoing replication and change data capture, in which you do an initial transfer of data, then repeatedly transfer changes to run two systems in parallel to ensure new system works

Step 1: Convert your schema

Source DB > AWS SCT > Target DB

Step 2: Move your data

Source DB > AWS DMS > Target DB

# Module 9: Database Architecture

